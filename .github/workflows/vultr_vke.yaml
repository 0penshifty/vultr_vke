name: Manage Vultr Kubernetes Engine

on:
  workflow_dispatch:
    inputs:
      clustername:
        description: 'Cluster Name'
        required: true
        type: string
      action:
        description: 'Action to perform'
        required: true
        default: 'deploy'
        type: choice
        options:
          - deploy
          - destroy

jobs:
  deploy:
    if: github.event_name != 'workflow_dispatch' || github.event.inputs.action == 'deploy'
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: 1.5.2

    - name: Setup Kubernetes Tools
      uses: yokawasa/action-setup-kube-tools@v0.11.1
      with:
        setup-tools: |
          kubectl
          helm

    - name: Authenticate to Vultr
      env:
        VULTR_API_KEY: ${{ secrets.VULTR_API_KEY }}
      run: |
        echo $VULTR_API_KEY > ~/.vultr_token

    - name: Initialize Terraform
      env:
        VULTR_API_KEY: ${{ secrets.VULTR_API_KEY }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: terraform init
      working-directory: ./terraform/cluster

    - name: Plan Terraform deployment
      env:
        VULTR_API_KEY: ${{ secrets.VULTR_API_KEY }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        TF_VAR_cluster_name: ${{ github.event.inputs.clustername }}
      run: terraform plan -out=tfplan
      working-directory: ./terraform/cluster

    - name: Apply Terraform deployment
      if: github.ref == 'refs/heads/main'
      env:
        VULTR_API_KEY: ${{ secrets.VULTR_API_KEY }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: terraform apply -auto-approve tfplan
      working-directory: ./terraform/cluster

    - name: Extract Kube Config 
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |
        mkdir -p ~/.kube
        terraform-bin -chdir="./terraform/cluster" output --raw vultr_kube_config | base64 -d > ~/.kube/config
        chmod 0600 ~/.kube/config

    - name: Wait for API to become available
      shell: bash
      run: |
        loopcounter=0
        while [[ $loopcounter -le 10 ]]; do
          kubectl get nodes >/dev/null
          if [[ $? -eq 0 ]]; then
            break
          else
            ((loopcounter++))
            echo "Waiting for Kubernetes API to become available"
            sleep 5
          fi
        done
        if [[ loopcounter -gt 10 ]]; then exit 1; fi

    - name: Install Cert-Manager
      shell: bash
      run: |
        kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.15.1/cert-manager.yaml
        kubectl create secret generic vultr-credentials --from-literal=apiKey=${{ secrets.VULTR_API_KEY }} --namespace cert-manager || echo "Secret already exists"
        git clone --depth=1 https://github.com/vultr/cert-manager-webhook-vultr.git ./vultr-webhook

        # Avoid a race condition where vultr webhook tries to connect before cert-manager-webhook is ready and fails install process
        loopcounter=0
        until [[ $(kubectl get deployments/cert-manager-webhook -n cert-manager -o jsonpath='{.status.readyReplicas}') -gt 0 && $loopcounter -lt 10 ]]; do
          ((loopcounter++))
          echo "Waiting for webhook dependency"
          sleep 5
        done
        if [[ $loopcounter -gt 10 ]]; then exit 1; fi

        ls -l ./vultr-webhook/deploy
        helm upgrade --install --namespace cert-manager cert-manager-webhook-vultr ./vultr-webhook/deploy/cert-manager-webhook-vultr
        cat <<EOF | oc apply -f -
        apiVersion: cert-manager.io/v1
        kind: ClusterIssuer
        metadata:
          annotations:
            meta.helm.sh/release-name: cert-manager-webhook-vultr
            meta.helm.sh/release-namespace: cert-manager
          labels:
            app.kubernetes.io/managed-by: Helm
          name: vultr-letsencrypt-prod
        spec:
          acme:
            email: ${{ secrets.admin_email }}
            privateKeySecretRef:
              name: vultr-letsencrypt-prod
            server: https://acme-v02.api.letsencrypt.org/directory
            solvers:
            - dns01:
                webhook:
                  config:
                    apiKeySecretRef:
                      key: apiKey
                      name: vultr-credentials
                  groupName: acme.vultr.com
                  solverName: vultr
        EOF
        cat <<EOF | oc apply -f -
        apiVersion: rbac.authorization.k8s.io/v1
        kind: Role
        metadata:
          name: cert-manager-webhook-vultr:secret-reader
          namespace: cert-manager
        rules:
        - apiGroups: [""]
          resources: ["secrets"]
          resourceNames: ["vultr-credentials"]
          verbs: ["get", "watch"]
        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: RoleBinding
        metadata:
          name: cert-manager-webhook-vultr:secret-reader
          namespace: cert-manager
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: cert-manager-webhook-vultr:secret-reader
        subjects:
          - apiGroup: ""
            kind: ServiceAccount
            name: cert-manager-webhook-vultr
        EOF
    
    - name: Install ExternalDNS
      shell: bash
      run: |
        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: external-dns
        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRole
        metadata:
          name: external-dns
        rules:
        - apiGroups: [""]
          resources: ["services","endpoints","pods"]
          verbs: ["get","watch","list"]
        - apiGroups: ["extensions","networking.k8s.io"]
          resources: ["ingresses"]
          verbs: ["get","watch","list"]
        - apiGroups: [""]
          resources: ["nodes"]
          verbs: ["list"]
        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          name: external-dns-viewer
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: external-dns
        subjects:
        - kind: ServiceAccount
          name: external-dns
          namespace: default
        ---
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: external-dns
        spec:
          strategy:
            type: Recreate
          selector:
            matchLabels:
              app: external-dns
          template:
            metadata:
              labels:
                app: external-dns
            spec:
              serviceAccountName: external-dns
              containers:
              - name: external-dns
                image: k8s.gcr.io/external-dns/external-dns:v0.10.2
                args:
                - --source=ingress  #service is also possible
                - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.
                - --provider=vultr
                - --registry=txt
                - --txt-owner-id=automation
                env:
                - name: VULTR_API_KEY
                  value: "${{ secrets.VULTR_API_KEY }}" # Enter your Vultr API Key
        EOF

    - name: Install Nginx Ingress Controller
      shell: bash
      run: |
        helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
        helm repo update
        helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx

    - name: Deploy ArgoCD Chart
      shell: bash
      run: |
        cat <<EOF >./argocd-values.yaml
        configs:
          cm:
            url: "https://argocd.${{ github.event.inputs.clustername }}/everythingkubernetes.info"
          params:
            server.insecure: "true"
        server:
          certificate:
            enabled: true
            secretName: argocd-server-tls
            domain: argocd.${{ github.event.inputs.clustername }}/everythingkubernetes.info
          issuer:
            group:  "acme.vultr.com"
            kind: "ClusterIssuer"
            name: "vultr-letsencrypt-prod"
        ingress:
          enabled: true
          annotations:
            external-dns.alpha.kubernetes.io/hostname: argocd.${{ github.event.inputs.clustername }}/everythingkubernetes.info
          ingressClassName: nginx
          hosts:
            - argocd.${{ github.event.inputs.clustername }}/everythingkubernetes.info
          tls:
            - hosts:
              - argocd.${{ github.event.inputs.clustername }}/everythingkubernetes.info
              secretName: argocd-server-tls
          https: false
        EOF
        helm upgrade --install --namespace argocd --create-namespace argocd argocd/argocd -f ./argocd-values.yaml

  destroy:
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'destroy'
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: 1.5.2

    - name: Authenticate to Vultr
      env:
        VULTR_API_KEY: ${{ secrets.VULTR_API_KEY }}
      run: |
        echo $VULTR_API_KEY > ~/.vultr_token

    - name: Initialize Terraform
      env:
        VULTR_API_KEY: ${{ secrets.VULTR_API_KEY }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: terraform init
      working-directory: ./terraform/cluster

    - name: Plan Terraform destruction
      env:
        VULTR_API_KEY: ${{ secrets.VULTR_API_KEY }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        TF_VAR_cluster_name: ${{ github.event.inputs.clustername }}
      run: terraform plan -destroy -out=tfdestroy
      working-directory: ./terraform/cluster

    - name: Apply Terraform destruction
      env:
        VULTR_API_KEY: ${{ secrets.VULTR_API_KEY }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: terraform apply -auto-approve tfdestroy
      working-directory: ./terraform/cluster

